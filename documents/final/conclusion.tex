\chapter{Conclusion}

We have shown that a system of cooperating web clients can reduce load on an origin server by automatically switching
from client-server file transfer to peer-to-peer content delivery. 
Experiments show that our system is capable of serving up to 30x faster than a traditional web server.
We have also found good settings for various system parameters.
A server timeout ($T$) setting of 0.75 seconds resulted in best performance, as did a bandwidth limit ($R$) of 160 KBps.
File block size of 32 KB was most effective, as was a linger time of 4 seconds or more and a peer connection count of 16 or more.
The system is most effective for small files but does not yet compete well with
BitTorrent for large files.  % nor for multiple files, except that that test was run with a low linger, so aggravated its own problems, and might still
% actually be quite fast, so don't mention it here.
The system met our goals of transparency to servers and ease of use, showing itself a viable option for users who wish to automate faster downloads.

% how could a linger of 4 be effective, when 20 versus 60 caused *so* much improvement?

Several aspects remain to be improved.  All clients currently connect to the origin
server, at times causing the same blocks to be served (redundantly) to various clients, slowing down transfer.
We also do not validate the integrity of downloads.  We assume peer trustworthiness and no file corruption.  We also do not provide peer incentives for sharing.
Peer lists could be optimized.  Because they are returned in chronological order from Bamboo (oldest first), our system can experience problems if peers
go offline without removing themselves from lists, or if linger times are close to expiring.  This also causes peers to download blocks 
from the the oldest 10 peers listed, which can cause unfairness. In the case of a slow Internet connection, linger times may have
already expired before peers even receive the list.

Under high loads DHT response times increase substantially.  For example, if there is an extremely popular file, the 
requests for that peer list might overwhelm the DHT member responsible.  There is also some redundancy in peer lists for large files.  
Clients request several different peer lists, one per block.  These lists might be all the same currently, thus
there is some avoidable redundancy and latency.

Currently this system is mostly useful for overloaded sites with static pages.  There could be some way for peers to deal with dynamic pages, such as to
store meta-data about which files are (or appear to be) static, and which are dynamic.  

Currently we used fixed system parameters, such as $R$, $W$, and $T$.  It may be useful to dynamically optimize these values.