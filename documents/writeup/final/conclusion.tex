
\section{Conclusion}
We have shown how to use a generic DHT to drive a scalable p2p web cache system, a valuable research objective.  A more practical question is when this system would be useful.  Flash crowds that are limited by origin bandwidth would be helped, because the static web objects would be shareable.  Downloaders of large files (ex: operating system updates) could be helped, as the automatic P2P aspect would alleviate load on the origin.  Groups of peers that are well connected could benefit (ex: within a large business several users download an OS update--sharing amongst themselves would speed download since it would be on the local intranet).  A server that is cpu bound would not benefit, since root pages are typically dynamic and must be generated by the origin server for each request.  A server that is extremely bandwidth limited (ex: serving from a DSL line).  Overall it appears to be useful to both servers and downloaders, though not in every situation.

A useful lesson learned is that OpenDHT at times can be quite slow, on the order of tens of seconds, sometimes even timing out after 90 or 200 seconds.  Sometimes one key out of 20 will take 20s or so while the rest will be fast.  This should be taken into account when building a system which uses it.  Another lesson is that OpenDHT appears to overload easily when multiple keys are simultaneously requested, so some balance must be found\footnote{With our system, we accomodated for the slow key lookup times by looking up several keys, all of which were redundant to the first.  Unfortunately this tended to overload gateways and slow overall lookup times}.

Our protocol could accomodate for it better.  For instance, currently peers list themselves under a single key per block.  This could cause the owner of that key to become overloaded.  It can also cause staleness if peers go offline without removing themselves from the list (currently they issue a remove command when their linger time is up--if the remove command fails or peers go offline, they remain on the lists in error).  A better algorithm for this could be designed and tested, for example one that queries keys based on time (like one for peers within the last 10s, one for peers within the last 100s, etc).  Also, Clients with a slow openDHT connection may receive peer lists several seconds after their request, meaning the list may already be outdated\footnote{For example, in a system where all peers have a small linger time, very possibly some can never connect to a live peer, because of the latency of requesting lists and the fact that the lists is retrieved in chronological order.  As another example, iff a client has a slow openDHT 'put' speed, it also can exceed its linger time before it even appears on the list of peers, and therefore will serve nothing.}.  One option to combat this might be a kind of subscribe system, in which peers without certain blocks list themselves, so that those who do get a certain block can find those peers and quickly connect to them and upload the block, before linger time expires, etc.

The DHT currently lists the oldest peers with a block first (even if they're actually no longer serving it).  This means we tend to use peers that are older, and more likely to exceed their linger time, especially if a peer has a slow openDHT connection.  We also tend to hammer the first few peers to do most of the sharing (very unfair).  A better system would be nice.  

We do not handle well the case of extreme flash crowds well (which is acceptable, they being rare).  We connect to the origin once from each peer (which partially explains our slowness compared to BitTorrent). Limiting the load on the origin server, or coordinating among peers of which bytes are requested, to avoid duplication, might be answers to this problem.

We also do not examine how to maximize the effectiveness of such a system for very small hosts, such as dial-up modem users.  Maybe it wouldn't help them, but maybe it wouldn't.

It should be noted that BitTorrent performs ``pre-requests'' for blocks [i.e. requesting the next sub-block before the current is done being downloaded].    HTTP/1.1 may not allow us this privilege, and it would be interesting to see if it is effective (or even the effect of re-using sockets, etc.)

Using HTTP (which we do currently) allows us to theoretically make our peers into back-up servers for (non aware) clients via redirection (ex: CoopNet did this).

We do not test this system with a true 'internet wide' test--i.e. many multiple files being requested simultaneously, and therefore it might be the case that OpenDHT doesn't scale well at all in such a state.

We use redundant requests currently--from the origin server plus the first peer used, and also for the last block.  There may be some way to reduce redundancy.

Note also that we did not try to use BitTorrent's DHT system, nor compare to it, but it might be useful as an alternate to OpenDHT.

Future goals might include ensuring the validity of files and providing explicit incentives.

We leave these for future work.
