\section{Conclusion}

We have shown how to use a generic DHT to drive a scalable p2p cache system, a useful research objective.  
A useful lesson learned is that OpenDHT can be quite slow for certain keys, sometimes even timing out after 90 or 200 seconds.  For example, sometimes 1 key out of 20 will take 20s or so (consistently) while the rest will be very fast.  This should be taken into account when building a system around OpenDHT.  Another lesson is that OpenDHT appears to overload easily when multiple keys are simultaneously requested from a single gateway \footnote{With our system, we accomodated for the slow key lookup times by looking up several keys, all of which were redundant to the first.  Unfortunately this tended to overload gateways and slow overall lookup times, so some balance should be found.}.
Our protocol could accomodate for this slowness better.  For instance, currently peers list themselves under a single key\footnote{as well as some duplicates per key} per block.  This can cause the owner of a key to become overloaded, and relies on the owner to be fast.  OPenDHT serves the peer lists in oldest to newest order, returning the oldest 9, then next oldest 9, then next.  This can cause staleness in our system if peers, for example, go offline without removing themselves from the list.  The list becomes stale with old abandoned peers.  A better algorithm for this could be designed.  This also causes peers to download the file from the first peers listed, causing unfairness\cite{Brian_Thesis}.  Also, clients with a slow openDHT connection may receive peer lists that are valid but have become outdated\footnote{For example, in a system where all peers have a small linger time, very possibly some slow peers could never connect to a live peer, because of staleness.  As another example, if a client has a slow openDHT 'put' speed, its linger time could expire before it is even listed on the peer list, and therefore will end up serving nothing.}.

A more practical question is when this protocol would prove useful if used in real life.  Flash crowds that are limited by origin bandwidth would benefit, because static web objects would be cached, however they would probably still be limited by CPU at the origin server, so would still fail.  Downloaders of large files (ex: operating system updates) would benefit, as the automatic P2P transition would alleviate load.  Groups of peers that download the same files could benefit (ex: within a large business several peers download the same file--sharing would speed download since it would be shared via local networks).  A server that is cpu bound would not benefit, since root pages are typically dynamic and must be generated by the origin server for each request.  A server that is extremely bandwidth limited (ex: a DSL line, or cross-country) would benefit.  Overall it appears to be useful to both servers and downloaders, though not the end-all of load alleviation.

As noted in the results section, we currently do not handle the case of extreme flash crowds (i.e. when hundreds of peers access a site without any of them having accessed it previously, in order to prime the system).  We assume that this situation is rare, however.  Because of this assumption, we currently connect to the origin once per peer (i.e. from all peers). Limiting the load on the origin server, or coordinating among peers to avoid duplication, might answer this problem.

We also do not examine how to maximize the effectiveness of such a system for very small hosts, such as dial-up modem users, nor did we examine if it is useful to a web site that is extremely well connected.

It should be noted that BitTorrent performs ``pre-requests'' for blocks [i.e. requesting the next sub-block before the current is done being downloaded].   HTTP/1.1 may not allow us this privilege, and it would be interesting to see if it is effective, and/or possible with HTTP/1.1.

We do not attempt a true 'internet scale' test--i.e. many multiple files being requested simultaneously, and therefore it might be the case that OpenDHT doesn't scale well in those conditions.

We use redundant requests currently for the last block, and also redundancy between the origin server and peers--there may be some way to reduce the waste caused by this (BitTorrent also suffers from this).

We did not try to use any alternative DHT, for example BitTorrent's mainline DHT system, but it might be interesting to examine it as an alternate to OpenDHT.

We do not ensure the validity of files (i.e. we assume peer trustworthiness and non file corruption), nor do we provide peer incentives for sharing.

We leave all these for future work.
