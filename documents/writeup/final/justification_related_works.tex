\section {Problem Introduction}
In the traditional Internet architecture, a single server handles multiple incoming client requests.  The bottleneck in such systems under high load can be lack of bandwidth to and from the server \cite{coopnet}, because bandwidth is split $N$ ways among the accessing clients (see Fig. \ref{fig:traditional_http}).  When an increasing number of users access a web page, that page therefore loads more and more slowly for users.  This problem occurs when server bandwidth is very low, a server experiences spikes in load, or multiple clients download very large files.  An example of this problem is the ``slashdot" effect, in which a sudden flash crowd of clients suddenly request certain pages, overwhelming the unexpectant server (for example when linked to by the slashdot site).  These situations cause web servers to become overwhelmed and possibly crash.

This problem is commonly combated by distributing the requests among a set of servers that mirror the content of the original server (Fig. \ref{fig:server_only}).  A company establishes mirrors by locating a set of servers in many different places on the Internet, then automatically redirecting clients to these servers, thus distributing the load to provide better performance.  An example of this is the large commercial CDN run by Akamai \cite{akamai}, which is employed to distribute large file downloads (for example Apple uses it to distribute their iTunes\texttrademark  program).  This solution however requires a dedicated pool of servers to provide the extra bandwidth.  Not all sites can afford a CDN, and the mirrors require cooperation, configuration, and maintenance, hence are expensive.

As a cheaper alternative, many web sites are beginning to use \emph{peer-to-peer content distribution}, or \emph{swarming}, with BitTorrent.  In BitTorrent, clients download only parts of a file from a central server, and get the rest of it from their peers in the system.  While a client participates, it is both downloading the blocks it needs and uploading the blocks it has to other peers (see Fig. \ref{fig:normal_swarm}).  Peer-to-peer content distribution like this enables a web server to serve a large file to large numbers of users without overloading its server bandwidth \cite{zappala}. Swarming protocols have been shown to even decrease total download time per peer as the number of peers increases \cite{slurpie}. 

\begin{figure}
\begin{center}
   \includegraphics[width=11cm]{description_pics/traditional_http.png}
    \caption{Traditional HTTP download}
 \label{fig:traditional_http}
 \end{center}
\end{figure}
\begin{figure}
    \centering
  \includegraphics[width=8cm]{description_pics/server_side_only.png}
  \caption{Server CDN example}
  \label{fig:server_only}
\end{figure}   
\begin{figure}
 \centering
 \includegraphics[width=4.5cm]{description_pics/normal_swarm.png}
 \caption{Swarming example.  Arrows represent block exchanges.}
 \label{fig:normal_swarm}
\end{figure}

BitTorrent currently requires setup for both servers and users, and isn't well-integrated into normal HTTP downloads.  BitTorrent peers must first locate a file with a ``.torrent'' extension from an out of band source such as a web server.  This file lists a target file's size, MD5 hashes of the blocks of the file, and the IP address of a tracker for that file.  This tracker is a dedicated machine which helps connect the peers to each another.  Peers contact the tracker to receive a list of peers who are currently downloading the file.  They then establish connections and begin sharing and receiving blocks with their neighbors.

%->final ?:BitTorrent, for the download of the last block of a file, uses a kind of 'many peers downloading the same blocks.  It requests the last block simultaneously from many peers, so that if one peer is transmitting it very slowly, they will be quickly passed by one of the faster seeds, and thus download it quickly.

BitTorrent is not commonly used for serving all the files of a web site.  It is used mostly for large files.  Every file shared has a .torrent file must be created, and a tracker established for that file\footnote{There have been some proposals of integrating BitTorrent with Apache \cite{webtorrent}, which show that even if a few clients contribute it helps with speedup, however this is mostly proposed as a server and client-side solution.  There are applets that will download BitTorrent files, and FireFox plugins to make downloading more seamless, but it still has a high barrier to entry for most users, and servers must understand the protocol.}.  This type of configuration is difficult for an entire web site, and unintuitive for small files.  Also, swarms are formed on a per file basis, which increases overhead when applied to multiple files.  BitTorrent also employys incentives to share blocks of files, which can cause longer boot-strapping time for new peers, which is suited to small file downloads.  Recent improvements to BitTorrent include a ``trackerless'' option, which means peer lookup is shared amongst the peers themselves, but access to the original .torrent files is still centralized, requires an additional cost of setup for the web server, and peers must be BitTorrent capable.  This design requires that all web servers and clients know and use a peer-to-peer protocol, causing a barrier to entry (for example, download sites offer links to BitTorrent downloads which are ignored by most users).  Our protocol modifies web clients only, so that they transition to peer-to-peer download for files on any web server.  This client-side approach seems more feasible for widespread distribution.  The trade-offs of client-side and server-side protocols is discussed in more depth in the related works section.

\section {Thesis Statement}\label{section:thesis}
Our hypothesis is that a system of cooperating web clients can reduce load on an origin web server by automatically switching from client-server file transfer to peer-to-peer content delivery.  This system can provide fast download times for objects of many sizes, especially small files.  It should be equivalent in speed to BitTorrent for larger files, without requiring any manual configuration or participation from central web servers.

\section{Related Work}\label{section:related_work}
Much work has been done to distribute file downloads.  Solutions fall into three basic categories: client-side protocols, client and server-side cooperative protocols, and server-side protocols.  Each category has certain trade-offs and challenges.

\subsection{Client-Side Only Protocols}
Protocols that use only client-side protocols are popular to help alleviate flash crowds.  Client-side interactions involve peers self-organizing to share content.  These solutions require no change in the server software, allowing them to be implemented in clients without requiring servers to be aware of the changes.  However, this requires that clients must self-organize without the help of the centralized server.  Our protocol is an example of such a system.

Shared web caches are an example of a client-side only protocol.  They allow peers to download files from some well known cache--typically a specific set of computers, such as those run by PlanetLab.  Coral \cite{coral} and CoDeen \cite{codeen} are examples of such caches.  Shared caches like these require a dedicated set of proxies, however.  Squirrel \cite{squirrel} is another shared web cache.  Squirrel clients join a local DHT and cache any files that map to their location in the DHT.  The DHT is used to locate the peer owner who then serves as proxy for that file.  The advantage of this is it doesn't require dedicated infrastructure.  The disadvantage is it does not offer an algorithm for transparent transition to p2p download, nor does it allow for partial file downloads.  It was also built on the assumption of being run on a local (well-connected) network, not on the Internet at large.

Coral-style caches have their place, but are limited.  They are constrained to the number of participating proxies, and they require an infrastructure of hosts who are willing to altruistically provide bandwidth, which is rare, because of expense.  Also, in extreme situations (load surpassing the bandwidth of the combined sum of procies), they will still become overburdened (as an example, if Coral receives a request for a file too many times, it begins redirecting peers to the origin server, thus not solving the original problem.).  Coral is also limited in the size of files it can cache by PlanetLab policies.  With our system this should not be a problem, as more peers means that more are available to upload.  These systems also lack an algorithm for transition to peer-to-peer download, and sharing of blocks among peers.  They also add an extra hop in network latency per request, even if a request from the origin server would have been the fastest way to download the file.    

%The effect of  flash crowds can also be alleviated by searching among a random subset of Internet peers for desired files.  PROOFS \cite{proofs} (P2P Randomized Overlays to Obviate Flash-crowd Symptoms) uses flooded search to allow peers to locate 'popular' or 'flash crowded' files from other peers who have previously downloaded the same.  The creators conjecture that files that are the subjects of a flash crowd are likely to be locatable among random sets of peers, since they are popular, hence their use of a random flood search.  The benefit of this system is that nodes need not maintain a structured search system (such as a DHT). However, not having a DHT for lookup makes searches more random and slightly less accurate, with potentially higher overhead.  PROOFS also does not include parallel downloads nor offer a protocol for automatic transition to a peer-to-peer solution.

Overall, client-side solutions alleviate flash-crowds, but are limited in certain situations. There is also little automatic transition to peer-to-peer (block-style) download.

\subsection{Server-side and Client-side Cooperative Protocols} 

The most popular type of load alleviation currently is a server and client-side cooperative protocol, BitTorrent.  The biggest disadvantage of this is it requires adoptance on both the server and client sides.

%Many file distribution protocols include some cooperation and previous knowledge between the server and client.  In these protocols clients typically access blocks of a file from the server and also from peers.  Server-side and client-side cooperative protocols basically fall into two categories: those used for web redirection, and those used to download single objects.  For web redirection, servers typically redirect clients to former clients that have downloaded files previously \cite{pseudoserving, coopnet}.  This allows a server to 'meter' its upload speeds and redirect peers, when appropriate, thus providing a backup strategy for over-loaded servers.  An example of this is the pseudo-server system \cite{pseudoserving}.  This style of protocol typically requires changes to both the server and client software, however, and the server could still become overloaded in extreme cases.

Some other related protocols incorporate web-redirection (i.e. the server redirects peers to recent downloaders), and also may or may not include block-sharing among peers, or swarming \cite{overhaul, webtorrent, onion, zappala, cohen, slurpie, mutualcast, fastreplica, avalanche, bullet_prime}.  Swarming provides the benefit of scalable downloads of large files.  OnionNetworks, for instance, proposes an extension to HTTP to allow HTTP response messages to include hashes of files and a list of peers from to which other peers may connect and download in a swarming fashion \cite{onion}.  Unfortunately their proposal has not seen wide-spread adoptance, nor research.  BitTorrent, the most commonly used swarming protocol, was developed in 2001 by Bram Cohen \cite{cohen}.  Swarming protocols are highly effective at serving loads orders of magnitude higher than an ordinary web server \cite{zappala}. Current protocols requires extra setup for both client and server, thus have a high barrier to entry.

The Shark \cite{shark} filesystem allows clients to download blocks of files from nearby neighbors who have blocks already on their local machines.  Shark is similar to our proposed system, except applied to files in a file system, not web objects.  It  requires a custom DHT for peer localization and proximity estimation, and a central server for hash values of files.  It also lacks HTTP integration and a switching mechanism.  Shark is quite similar to our design, though designed for file systems.

Some recent proposals integrate HTTP clients with BitTorrent itself.  In these solutions a user is presented the option of downloading via HTTP or via BitTorrent (swarming), depending on which they think will give them the quicker download.  This provides a backup for overloaded servers.  To the users this requires a manual choice between normal to swarming download (and almost always p2p download is slower, in the experience of the author, thus the choice is moot.).  The Osprey system \cite{osprey} is an ftp server that automatically generates .torrent files for all files in an ftp sub-directory, and integrates a BitTorrent tracker into the server software to handle the different types of requests.  These systems allow peers to manually switch to swarming if the server load becomes too high.  There has also  been development in plugins for web browsers to ease downloading of BitTorrent files.

Dijjer \cite{dijjer} is a tool that automatically performs swarming downloads of any file.  If passed a url like http://dijjer.org/get/http://mysite.com/video.mov the request is intercepted by the locally running Dijjer software, which performs a distributed download of the file.  One can also right-click on any arbitrary file and select 'download via Dijjer' for the same effect.  Dijjer contacts a quasi-DHT (similar to Freenet \cite{freenet}) for block hashes and downloads the blocks from random peers who cache these blocks.  Unfortunately, Dijjer lacks automation in the transition to download, and, as the Dijjer software is currently implemented, requires peers to cache material in which they were never interested, so is slightly intrusive, for example causing clients to potentially cache illegal or suspect web objects.  Also their DHT is non deterministic, which might be less than optimal.

Overall, client and server side cooperation protocols work well at alleviating flash crowds, and some provide automation for transition to p2p delivery.  However, they require both the server and client to understand the protocol, which hinders their adoptability.  The biggest drawback of these protocols is that they are not transparent to one side or the other.

\section {Proposed Solution}\label{section:solution}
Our proposed solution emphasizes a client-side system that automatically switches from client-server to a peer-to-peer content delivery without any manual per-file configuration for either clients or servers.  The goals for our system include:
\begin{enumerate}
\item It should be transparent to servers.  This allows the origin servers to remain unchanged, so end users can benefit from the system despite using servers who know nothing about it.
\item It should appear transparent to users by automatically transitioning to peer-to-peer content delivery when the server is slow.
\item It should not require a dedicated special-purpose infrastructure.  Using a general-purpose infrastructure, coupled with a dependence on the clients for transferring blocks, makes this system easier to adopt.
\item It should be non-intrusive, in that peers should not be required to cache blocks of files in which they were never interested.  Peers will avoid caching files they never downloaded, and will not be responsible for content they don't anticipate.  Users would also only be using their upload bandwidth for files in which they are interested, encouraging participation and adoption.
\item It should be fast for small files.
\end{enumerate}

%A fundamental design decision is also whether to download the contents of blocks from peers or from the DHT itself; peers could store the block contents, and have the DHT serve as a lookup of peers, or could store the block contents on the DHT itself.  The trade-off is that having the blocks on the DHT puts more stress on the DHT and relies on members of the DHT for contributing bandwidth, which is intrusive.  Imagine for instance caching portions of a file on your personal computer which you never downloaded for yourself, and which you are then sharing with others.  Members of the DHT would thus intrusively cache information for which they are not interested.  We therefore choose the case of having peers register themselves on a DHT as being willing to serve blocks they own, and have clients download directly from peers.  This seems more indicative of a realistic web experience.

