%% note: todo's go in todo, as well as optionally in here

\section{Experiment Results}
For our initial experiment we first run tests to see if the system works effectively.  We download a small file (100KB) with an increasing number of clients, to see how well it responds.
To generate different tests we run experiments for 100s, each time increasing the total number of peers entering during those 100s.  Each enters randomly within that time frame using a poisson distribution for timing.  The origin server is located on a well provisioned machine at BYU, uses an apache 2 web server that it bandwidth limited (using mod\_bw) to 256KB/s.  Peers downloading the file are randomly selected from available planetlab peers (a pool of up to 300 scattered world-wide on the planet-lab system).  Experiments run until all peers finish downloading the file.  Each experiment is repeated 3 times and results are averaged.  Because a few peers are extremely poorly connected and create a long tail for statistics, measures of aggregate throughput ignore the last 20% of the test (since during that time the throughput is close to zero).  Because at the beginning of tests very few peers are downloading the file (it hasn't reached steady-state), the first 20% is also ignored. Peers linger, serving the file, for 20 seconds after a download.

% todo double check 20% note there

We first run tests using traditional client-server transfer, to establish a base line from which to compare ourselves.  Client-server download times increase as expected--almost linearly with load; outgoing bandwidth is divided among all incoming clients (Fig.~\ref{figs:client_server_download_times}).

<%= figure 'pics/vr_unnamed316651_cs_stress_test/client_download_PercentileLine.pdf',
:label => 'figs:client_server_download_times', :caption => 'Traditional client server download times' %>

<%= figure 'pics/vr_unnamed316651_cs_stress_test/server_speed_PercentileLine.pdf',
:label => 'figs:client_server_server_load', :caption => 'Client Server load on the origin server' %>

We next test our system against the same load.  For default parameters, we set the file's block sizes to 100KB, set origin minimum allowable speed (R) to 256KB/s, (W) to 2 seconds, and first-byte timeout (T) to 1s.  

Download time of our system stays constant at around 10s, regardless of load, thus easily surpassing our initial speed goal of a 2x increase (Fig. \ref{figs:yanc_download_times}).

<%= figure 'pics/vr_medium_p2p_load_tak4/client_download_PercentileLine.pdf',
:label => 'figs:yanc_download_times', :caption => 'P2P Download Times' %>

Server load actually decreases under a higher load (Fig. \ref{figs:yanc_server_load}), as more peers serve the file, though the server still serves to some peers.

<%= figure 'pics/vr_medium_p2p_load_tak4/server_speed_PercentileLine.pdf',
:label => 'figs:yanc_server_load', :caption => 'P2P Server Load'%>

Also correlated to that is the increasing role that P2P transfer has with higher load.  Each peer downloads the file both from the origin and its peers, however, after about 10 peers/second almost 100\% of the file serving is being done via p2p (Fig.~\ref{figs:yanc_from_client_percentile}).
<%= figure 'pics/vr_medium_p2p_load_tak4/percent_from_clients_PercentileLine.pdf', :label => 'figs:yanc_from_client_percentile', :caption => 'P2P percent received from peers' %>

%% TODO could put in some cool graphs of a single run of p2p vs. cs--at least the first few files that is quite interesting :)

%% TODO we never got the stats for > 25 peers/s for yanc transfer

%% TODO re run this p2p load test--how did we get such dang good DHT times?

\section{Varying metrics}
\subsection{Time to wait for first byte before transitioning to P2P}
Theoretically, if a peer can download a file at "full speed" from the origin server, it does not help to also try to download the file from peers at the same time.  Our system attempts to allow for this by specifying parameters for the conditions under which it will switch to a mixed (p2p + origin) style download.
We test for the impact these settings have on download speeds, in order to explore the dynamics of the system.
We first test varying T, the time to wait for a first byte of a response before transitioning to P2P
\footnote{By first byte we mean any byte received from the origin server--a header byte or a content byte--i.e. we exclude the TCP handshake packet for our experiment.}.  
We expect that an extremely low value for this would cause transitioning too early and an
extremely high value would cause transitioning too late, both being
sub optimal.  The results show just that (Fig. \ref{figs:dt_variance_download_times}).  Also interesting is that as T
was increased, the peers download a higher percentage of the file from the origin (Fig. \ref{figs:dt_variance_percent_from_clients}).

<%= figure 'pics/vr_do_dts_take6/client_download_PercentileLine.pdf', :label => 'figs:dt_variance_download_times', :caption => 'Download times by varying T, the time to wait for first byte' %>

<%= figure 'pics/vr_do_dts_take6/percent_from_clients_PercentileLine.pdf', :label => 'figs:dt_variance_percent_from_clients', :caption => 'Percent downloaded from client, varying T' %>

\subsection{Server minimum allowed speed}
We also experimented with varying R, the server minimum allowed speed. This value is calculated by tracking the amount received over the previous W seconds.  If this rate falls below R, then we transition to a P2P style transfer.  We first run this experiment by varying R from 32KB/s to 1MB/s, and leave T set to 1s, a reasonable value.  The results from this first run were almost always flat (Fig. \ref{figs:dr_download_time}).  It turns out that R is not making a difference at all--the T parameter of one second caused almost all clients to transition immediately, thus making R not as useful.

<%= figure 'pics/vr_dr_take_8/client_download_Percentile_Line.pdf', :label => 'figs:dr_download_time', :caption => "Download times by varying R, the receive speed threshold" %>

\subsection{Server speed average}
We next vary W, the seconds of history to track to calculate R.  Our hypothesis is that a W setting that is too short would be inaccurate.  Surprisingly, this wasn't the case.  A smaller value for W is more effective, presumably because it alerted the peers more quickly to a potential server slowdown.  (Fig. \ref{figs:vr_dw_download_times}).
<%= figure 'pics/vr_dw_lots_opendht/client_download_Percentile_Line.pdf', :label => 'figs:vr_dw_download_times', :caption => 'Download times by varying W' %>

\section{Effect for a full web site}
We next run an experiment that is more indicative of real world performance.  The normal pattern for internet use it to access some root web page (typically dynamic) which contains multiple links to other small (mostly static) objects, such as images, javascript, flash, etc.  This is the case for the BYU current web site, which links to over 10 [mostly tiny] other objects.  We therefore run an experiment to see how our protocol fares in the case of downloading one page followed by several other, smaller pages.  The expected result is that downloading multiple pages will be slower than downloading a single page since it will put more stress on the DHT, which proved correct (Fig. \ref{figs:multiple_files_p2p_download_times_all_files}).
Download time (the download time for all files to complete downloading) appears to grow exponentially, and at about 15 peers/second reach 20 seconds total.  Also interesting is that DHT lookup time also grew at about the same scale (Fig. \ref{figs:multiple_files_p2p_dht_remove}).  We thus infer that slower DHT lookup time causes the system to slowdown, and that OpenDHT scales poorly under higher load.

<%= figure 'pics/vr_multiples_take_1/client_download_Percentile_Line.pdf', :caption => 'P2P Multiple Files end download times', :label => 'figs:multiple_files_p2p_download_times_all_files' %>

<%= figure 'pics/vr_multiples_take_1/dht_Remove_Percentile_Line.pdf', :caption => 'P2P multiple files DHT key value removal times', :label => 'figs:multiple_files_p2p_dht_remove' %>

To make sure our results were still better than client-server, we ran the same tests without any P2P transfer.  Download times grew at a much sharper rate (Fig. \ref{figs:multiple_files_cs_download_times}).  Server throughput appears to slow with higher load (Fig. \ref{figs:multiple_files_cs_server_rate}).

<%= figure 'pics/vr_unnamed937328_multiple_files_cs/client_download_Percentile_Line.pdf', :caption => 'Client-server multiple files download times', :label => 'figs:multiple_files_cs_download_times' %>
<%= figure 'pics/vr_unnamed937328_multiple_files_cs/server_speed_Percentile_Line.pdf', :caption => 'Client-server multiple files server speed', :label => 'figs:multiple_files_cs_server_rate' %>

\subsection{Varying Block Size}
We next measured the effect of block size.  32KB blocks were shown to be effective in \cite{TODO}.  This also was shown in our own results, with 32K blocks resulting in the quickest downloads (Fig.  \ref{figs:block_size_download_times}).

<%= figure 'pics/vr_unnamed240998_blockSize/client_download_Percentile_Line.pdf', :caption => 'Vary block size', :label => 'figs:block_size_download_times' %>

\section{Downloading Large Files}
We next want to compare ourselves to BitTorrent, famous for its speeds of downloading large files.  We have the two protocols download a single 30MB file from 100 simultaneous peers, and calculate the 1st, 25th, 50th, 75th, and 99th percentile download times\footnote{We used a block size of 256K for both protocols}.  As expected, BitTorrent shared a 30MB file fairly well.  Its median download time was 148.32s, with percentiles of: 131.11s 139.3s 148.32s 163.5s and 2786.08s.  Our p2p protocol yields a median time of 847.1s, with percentiles: 613.82s 730.61s 847.1s 882.51s and 982.67s respectively--not as fast for any but the 99th percentile, where it is much faster.  One cause of this is that BitTorrent seeds limit their outgoing connections to a few peers at a time, thus enabling blocks to propagate more quickly from peer to peer.  In the BitTorrent test, the origin server wasn't an apache instance, but it was a BitTorrent seed, so it limited its outgoing connections more aggressively.  In an attempt to force our clients to download from the origin with less frequency, we ran tests forcing each P2P client to only download one block at a time from the origin server.  This doesnt't fix the problem (presumably because it still had 100 connections on the origin), but did show an interesting, unrelated result--that a higher number of concurrent peers increased download speed (Fig. \ref{figs:large_file_number_of_concurrent_blocks}).  This test was of a flash flood downloading a large file, where our protocol falls short.  We would assume that in normal conditions there are a few peers entering the system before the flash flood occurs, in which case our protocol would work much better.  We leave that examination for future work.
\footnote{Another difference between BitTorrent and our protocol is that we currently relook up the list of peers per block, even if our current peers are listed on more than one of those lists.  Some form of gossip to determine which known peers have which blocks might decrease network overhead.}

<%= figure 'pics/vr_vary_blocks_large_file/client_download_Percentile_Line.pdf', :caption => 'Vary number of concurrent blocks in a large file', :label => 'figs:large_file_number_of_concurrent_blocks' %>

\section{Current limitations}
Currently our system saves the list of peers per block all in a single key.  This could cause the owner of that single key to become overloaded if the crowd becomes too large.  Also it can cause staleness of the list if dead peers go offline without removing themselves from the list first (currently they issue a remove command when their linger time is up).  A better algorithm for this could be designed and tested, for example one that queries keys based on time (like one for peers within the last 10s, one for peers within the last 100s, etc).

We also currently still connect to the origin from each peer, causing ourselves to be slower than BitTorrent.  Either way we are still twice as fast as client-server, so still consider ourselves successful at proving the hypothesis of faster small file downloads using P2P.

Multiple file grew exponentially--we need to figure out a way to avoid that!
