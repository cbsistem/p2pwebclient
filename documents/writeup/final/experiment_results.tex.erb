%% note: todo's go in todo, as well as optionally in here

\section{Experiment Results}
For our initial experiments we first test how our system scales.  We download a small file (100KB) with an increasing number of clients, to see how download times are affected by load.
We run several experiments, varying the number of peers who enter the system within the first 100s.  Peers enter the system with a frequency given by poisson distribution with a rate such that by 100s they will all have entered the system, so randomly, but with inter arrival times such that all peers enter within 100s.  The origin server is located on a well provisioned machine at BYU, using an apache2 web server that it bandwidth limited (using mod\_bw) to 256KB/s.  Peers are randomly selected from available PlanetLab hosts (a pool of up to 300 scattered world-wide on the PlanetLab system).  Experiments run until all peers finish downloading the file.  Each experiment is repeated 3 times and results are averaged.  Because a few peers are extremely poorly connected and create a long tail for aggregate statistics, measures of aggregate throughput ignore the last 20\% of the test time (since during that time the throughput is close to zero).  Because, at the beginning of tests, very few peers are downloading the file and it hasn't reached steady-state, the first 20\% is also ignored for aggregate statistics.  We measure the download times of all peers in a run, and display the 1st, 25th, 50th, 75th, and 99th percentiles.  We also measure the amount of load on the origin server, given by the sum of bytes received within a given second, by peers, from the origin.  Thus this only displays packets received from the origin, not all those sent.  We also calculate the percentage of the file received from peers versus from the origin, and graph its percentiles.  We also calculate the percentiles of the time it took to do openDHT get, put, and remove commands over the course of the entire run.

We first test a traditional client-server transfer, to establish a base line from which to compare our system.  Fig.~\ref{figs:client_server_download_times} shows the download times for clients as load increases.  It starts with a median download time of 1.33s with 1 peer entering the system/second, and grows to a median of 344s at 25 peers/second.  Client-server download times increase approximately linearly because we start a fixed number of peers and let them all complete their downloads.  Presumably if we continued the tests for longer we would see more of an exponential curve at about 5 peers/s. The topmost outliers at 20s were peers which typically waited in line almost 900s before finally being served the file.  Fig.~\ref{figs:client_server_server_load} shows the load on the origin server over time.  It grows quickly to the theoretical cap of 250KB/s. then actually decreases to 203KB/s at 20 peers/second.  It shows the limitations of our bandwidth limiter in that it becomes bursty at higher connection rates, because it can't keep track of lost connections well, but must let them time out.

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/client_download_Percentile_Line.pdf}
      \label{figs:client_server_download_times}      
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/server_speed_Percentile_Line.pdf}
      \label{figs:client_server_server_load}
    }
    \caption{Traditional client server download}  
  \end{center}
\end{figure*}


We next test our system under the same load.  For system parameters, we set reasonable defaults of block size of 100KB, origin minimum allowable speed (R) of 256KB/s, R's calculation window (W) of 2s, and the first-byte timeout (T) of 1s.  Peers linger, serving the file, for 20 seconds after completing a download.  Download median times(Fig. \ref{figs:yanc_download_times}) start at 1.4s/peer at a rate of 1 peer entering the system/s, and grow to 7.46 with 20 peers/s, and 8.21s with 25 peers/s, thus it is scaling much better.  Server load actually seems to decrease under a higher load (Fig. \ref{figs:yanc_server_load}), as more peers serve the file.  One reason for this is that, after a high enough load, apache appears to not respond to requests for a few seconds, as it has a connection limit of 256 by default.  Thus, when the system hits this limit, new peers by default timeout (T) on the origin server, and move to a p2p download.  This situation is also aggravated by a bug in mod\_bw, which effectively cuts off speeds under high load because it thinks more connections exist than do, until connections timeout.

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/client_download_Percentile_Line.pdf}
      \label{figs:yanc_download_times}
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/server_speed_Percentile_Line.pdf}
      \label{figs:yanc_server_load}
    }
    
    \subfigure[CDF of percent received from peers] {
      \includegraphics[width=70mm,]{pics/vr_medium_p2p_load_tak4/percent_from_clients_Percentile_Line.pdf}
      \label{figs:yanc_from_client_percentile}
    }
    
    \caption{P2P Download}    
  \end{center}
\end{figure*}

Also related is the increasing role of P2P transfer.  Under low load peers tend to download the file strictly from the origin, however, after about 10 peers/s almost 100\% of the transfer is being done via p2p (Fig.~\ref{figs:yanc_from_client_percentile}), validating our hypothesis that load can be lessened on the origin server.  A 1 on the graph means that 100% of the file transfer was from peers, instead of from the origin.  One reason that it goes up to 100\% so quickly is that we have only one block per file.  The reason it isn't always at 100\% is that we first start downloading from the origin, and, when we find peers, the peer quickly serves up what the origin was serving to us and completes the block.  So we get some from the origin and some from peers.
<%#= figure 'pics/vr_medium_p2p_load_tak4/percent_from_clients_Percentile_Line.pdf', :label => 'figs:yanc_from_client_percentile', :caption => 'CDF of percent received from peers instead of from the origin. A 1 means 100\% of transfer is being done via P2P.' %>

\section{Impact of various parameters}

Theoretically, if a peer can download at "full speed" from the origin server, it does not help to download the same file from peers.  Our system attempts to allow for this by specifying parameters for the conditions under which it will switch to a p2p download.  It should be noted that it switches to a mixed download, since it still downloads from the origin server as a backup.
We test the impact of varying each parameter, in order to explore the dynamics of the system, and search for optimal values.  In each test we run 1000 peers at 15/s (approximately 5 times the origin server's bandwidth for 66s).  

\subsection{Time to wait for first byte before transitioning to P2P}

We first vary $T$, the time to wait for a first response byte before transitioning\footnote{By first byte we mean any byte received from the origin server--a header byte or a content byte--i.e. we exclude the TCP handshake packet as counting as a first byte received.}.  
We expect that an extremely low value will cause transitioning too early and an extremely high value will cause transitioning too late.  The results confirm this (Fig. \ref{figs:dt_variance_download_times}).  Also, as T grew very large, the peers download a higher percentage of the file from the origin, as expected (Fig. \ref{figs:dt_variance_percent_from_clients}).

<%= figure 'pics/vr_do_dts_take6/client_download_Percentile_Line.pdf', :label => 'figs:dt_variance_download_times', :caption => 'Download times varying T, the time to wait for first byte' %>

<%= figure 'pics/vr_do_dts_take6/percent_from_clients_Percentile_Line.pdf', :label => 'figs:dt_variance_percent_from_clients', :caption => 'Percent downloaded from client, varying T' %>

\subsection{Server minimum allowed speed R}

We next vary $R$, the server minimum serving speed. This value is calculated by averaging the amount received over the previous W seconds.  We run this experiment by varying R from 32KB/s to 1MB/s, and leave T set to 1s, a reasonable value.  The results from this first run were almost flat (Fig. \ref{figs:dr_download_time}).  It turns out that R is not making a difference at all--the T parameter of one second causes almost all clients to transition immediately, thus surprisingly causing R to not be as useful as expected.

<%= figure 'pics/vr_dr_take_8/client_download_Percentile_Line.pdf', :label => 'figs:dr_download_time', :caption => "Download times by varying R, the receive speed threshold" %>

TODO re-do with better settings

\subsection{Server speed average W}

TODO vary W section--I did this but the results were corrupted as I had mis-set the server for some reason

%We next vary W, the seconds of recent history to track R.  Our hypothesis is that a too small W will be too sensitive and thus inaccurate.  Surprisingly, this didn't appear to be the case.  A smaller value for W is more effective, presumably because it alerted peers more quickly to potential server slowdown.  (Fig. \ref{figs:vr_dw_download_times}).
<%#= figure 'pics/vr_dw_lots_opendht/client_download_Percentile_Line.pdf', :label => 'figs:vr_dw_download_times', :caption => 'Download times by varying W' %>

\section{Effect for a full web page}
We next run an experiment more indicative of real-world situations.  The normal pattern for a webpage is to first access some root (typically dynamic) page which references several other (typically static) objects, such as images, javascript, flash, etc.  This is the case for the BYU web site\footnote{Snapshot 2007}, which has a medium sized main page that links to over 10 small other objects.  We therefore run an experiment to see how well our protocol downloads a small page followed by several other, smaller pages.  The expected result is that downloading multiple pages will be slower than downloading a single page since it will put more stress on the DHT, which proved correct (Fig. \ref{figs:multiple_files_p2p_download_times_all_files}).
Download time (the download time for all files to complete downloading) appears to grow exponentially, and at about 15 peers/second reach 20s total.  Also interesting is that DHT latency also grew with a similar curve (Fig. \ref{figs:multiple_files_p2p_dht_remove}).  We thus infer that slower DHT times cause the system to slow down, and that OpenDHT scales somewhat poorly under higher load.

<%= figure 'pics/vr_multiples_take_1/client_download_Percentile_Line.pdf', :caption => 'P2P Multiple Files end download times', :label => 'figs:multiple_files_p2p_download_times_all_files' %>

<%= figure 'pics/vr_multiples_take_1/dht_Remove_Percentile_Line.pdf', :caption => 'P2P multiple files DHT keyed value removal times', :label => 'figs:multiple_files_p2p_dht_remove' %>

We next run the same multiple file test using a traditional client-server download.  Download times grew at a much steeper rate (Fig. \ref{figs:multiple_files_cs_download_times}).  Server speed appears to actually slow with higher load (Fig. \ref{figs:multiple_files_cs_server_rate}).  Ours scaled much better than client-server.

<%= figure 'pics/multiples_p2p_versus_cs_pics/client_download_Percentile_Line.pdf', :caption => 'Comparison of P2P versus client server download times for multiple files', :label => 'figs:multiple_files_cs_download_times' %>
<%= figure 'pics/vr_unnamed937328_multiple_files_cs/server_speed_Percentile_Line.pdf', :caption => 'Client-server multiple files server speed', :label => 'figs:multiple_files_cs_server_rate' %>

\subsection{Varying Block Size}
We next measure the effect block size has on downloads.  
%32KB blocks were shown to be effective in \cite{TODO}.  
32K blocks resulted in the quickest downloads (Fig.  \ref{figs:block_size_download_times}), though not by much.

<%= figure 'pics/vr_unnamed240998_blockSize/client_download_Percentile_Line.pdf', :caption => 'Vary block size', :label => 'figs:block_size_download_times' %>

\section{Downloading Large Files}
We next download large files.  We download a single 30MB file from 100 simultaneous peers with our protocol and with BitTorrent.  Block size is set to 256K for both systems (the BitTorrent default).  100 peers enter the system within 100s, and both protocols are rate limited to 256KB/s.  The expected result is that the two fare similarly.  BitTorrent download times were 148s for the 50th percentile, and 2786s for the 99th percentile.  Our protocol had a median download time of 847s, with a 99th percentile of 982s, so our currently is not as fast as BitTorrent.  One cause of this difference is that BitTorrent's seeds (servers) limit the number of outgoing connections they will serve to, enabling blocks to propagate more quickly to connected peers and thus from them out through the system\footnote{BitTorrent seeds limit their outgoing connections to 10 or less, whereas our apache was tooled to serve up to 256 clients at the same time}.  BitTorrent seeds also prefer peers who have the highest download speeds, which speeds propagation of blocks.

Also, this test was basically of a flash flood downloading a large file, which is rare.  In normal conditions a few "seeder" peers enter the system before floods occur, in which case our protocol would be expected to work better.  We leave that examination for future work.

\section{Number of peers} We next vary the number of peers that each peer downloads from.  We use the same 30MB file size.  A higher number of concurrent peers increases download speed (Fig. \ref{figs:large_file_number_of_concurrent_blocks}).  
<%= figure 'pics/vr_vary_blocks_large_file/client_download_Percentile_Line.pdf', :caption => 'Download times varying number of concurrent peers', :label => 'figs:large_file_number_of_concurrent_blocks' %>
