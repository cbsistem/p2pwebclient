%% note: todo's go in todo, as well as optionally in here

\section{Experiment Results}
For our initial experiment we first run tests to see if the system works effectively.  We download a small file (100KB) with an increasing number of clients, to see how well it responds.
To generate different tests we run experiments for 100s, each time increasing the total number of peers entering during those 100s.  Each enters randomly within that time frame using a poisson distribution for timing.  The origin server is located on a well provisioned machine at BYU, uses an apache 2 web server that it bandwidth limited (using mod\_bw) to 256KB/s.  Peers downloading the file are randomly selected from available planetlab peers (a pool of up to 300 scattered world-wide on the planet-lab system).  Experiments run until all peers finish downloading the file.  Each experiment is repeated 3 times and results are averaged.  Because a few peers are extremely poorly connected and create a long tail for statistics, measures of aggregate throughput ignore the last 20% of the test (since during that time the throughput is close to zero).  Because at the beginning of tests very few peers are downloading the file (it hasn't reached steady-state), the first 20\% is also ignored. Peers linger, serving the file, for 20 seconds after a download.  We calculate the 1st, 25th, 50th, 75th, and 995h percentiles of various metrics (download speed, opendht lookup times, etc.).

% todo double check 20% note there

We first run tests using traditional client-server transfer, to establish a base line from which to compare ourselves.  Client-server download times increase as expected--almost linearly with load; outgoing bandwidth is divided among all incoming clients (Fig.~\ref{figs:client_server_download_times}).

<%= figure 'pics/vr_unnamed316651_cs_stress_test/client_download_PercentileLine.pdf',
:label => 'figs:client_server_download_times', :caption => 'Traditional client server download times' %>

<%= figure 'pics/vr_unnamed316651_cs_stress_test/server_speed_PercentileLine.pdf',
:label => 'figs:client_server_server_load', :caption => 'Client Server load on the origin server' %>

We next test our system with the same loads.  For default parameters, we set the file's block sizes to 100KB, the origin minimum allowable speed (R) to 256KB/s, (W) to 2 seconds, and the first-byte timeout (T) to 1s.

The download times of peers stays constant at around 10s, regardless of load, surpassing our initial speed goal of a 2 fold increase (Fig. \ref{figs:yanc_download_times}).

<%= figure 'pics/vr_medium_p2p_load_tak4/client_download_PercentileLine.pdf',
:label => 'figs:yanc_download_times', :caption => 'P2P Download Times' %>

Server load actually seems to decrease under a higher load (Fig. \ref{figs:yanc_server_load}), as more peers serve the file, though the server still serves to some peers\footnote{This may have been caused by a bug in mod\_bw, which effectively cut off speeds under high load}.

<%= figure 'pics/vr_medium_p2p_load_tak4/server_speed_PercentileLine.pdf',
:label => 'figs:yanc_server_load', :caption => 'P2P Server Load'%>

Also related is the increasing role that P2P transfer has with higher load.  Under low load peers tended to download the file strictly from the origin, however, after about 10 peers/second almost 100\% of the file serving is being done via p2p (Fig.~\ref{figs:yanc_from_client_percentile}).
<%= figure 'pics/vr_medium_p2p_load_tak4/percent_from_clients_PercentileLine.pdf', :label => 'figs:yanc_from_client_percentile', :caption => 'P2P percent received from peers' %>

%% TODO could put in some cool graphs of a single run of p2p vs. cs--at least the first few files that is quite interesting :)

%% TODO we never got the stats for > 25 peers/s for yanc transfer

%% TODO re run this p2p load test--how did we get such dang good DHT times?

\section{Varying metrics}
\subsection{Time to wait for first byte before transitioning to P2P}
Theoretically, if a peer can download a file at "full speed" from the origin server, it does not help to also try to download the file from peers at the same time.  Our system attempts to allow for this by specifying parameters for the conditions under which it will switch to a mixed (p2p + origin) style download.
We test for the impact these settings have on download speeds, in order to explore the dynamics of the system.
We first test different values for T, the time to wait for a first byte of a response before transitioning to P2P
\footnote{By first byte we mean any byte received from the origin server--a header byte or a content byte--i.e. we exclude the TCP handshake packet for our experiment.}.  
We expect that an extremely low value for this would cause transitioning too early and an
extremely high value would cause transitioning too late, both being
sub optimal.  The results show just that (Fig. \ref{figs:dt_variance_download_times}).  Also interesting is that as T
was increased, the peers download a higher percentage of the file from the origin (Fig. \ref{figs:dt_variance_percent_from_clients}).

<%= figure 'pics/vr_do_dts_take6/client_download_PercentileLine.pdf', :label => 'figs:dt_variance_download_times', :caption => 'Download times by varying T, the time to wait for first byte' %>

<%= figure 'pics/vr_do_dts_take6/percent_from_clients_PercentileLine.pdf', :label => 'figs:dt_variance_percent_from_clients', :caption => 'Percent downloaded from client, varying T' %>

\subsection{Server minimum allowed speed}
We also experimented with varying R, the server minimum allowed speed. This value is calculated by tracking the amount received over the previous W seconds.  If this rate falls below R, then we transition to a P2P style transfer.  We first run this experiment by varying R from 32KB/s to 1MB/s, and leave T set to 1s, a reasonable value.  The results from this first run were almost always flat (Fig. \ref{figs:dr_download_time}).  It turns out that R is not making a difference at all--the T parameter of one second caused almost all clients to transition immediately, thus making R not as useful.

<%= figure 'pics/vr_dr_take_8/client_download_Percentile_Line.pdf', :label => 'figs:dr_download_time', :caption => "Download times by varying R, the receive speed threshold" %>

\subsection{Server speed average}
We next vary W, the seconds of recent history to use to track R.  Our hypothesis is that too small of a W value will be too sensitive and be inaccurate.  Surprisingly, this didn't appear to be the case.  A smaller value for W is more effective, presumably because it alerted the peers more quickly to a potential server slowdown.  (Fig. \ref{figs:vr_dw_download_times}).
<%= figure 'pics/vr_dw_lots_opendht/client_download_Percentile_Line.pdf', :label => 'figs:vr_dw_download_times', :caption => 'Download times by varying W' %>

\section{Effect for a full web page}
We next run an experiment that is more indicative of real-world downloads.  The normal pattern for an internet page is to access some root web page (typically dynamic) which references several other small (mostly static) objects, such as images, javascript, flash, etc.  This is the case for the BYU web site\footnote{Snapshot in 2007}, which has a medium sized main page that links to over 10 small other objects.  We therefore run an experiment to see how well our protocol downloads one page followed by several other, smaller pages.  The expected result is that downloading multiple pages will be slower than downloading a single page since it will put more stress on the DHT, which proved correct (Fig. \ref{figs:multiple_files_p2p_download_times_all_files}).
Download time (the download time for all files to complete downloading) appears to grow exponentially, and at about 15 peers/second reach 20 seconds total.  Also interesting is that DHT latency also grew at about the same scale (Fig. \ref{figs:multiple_files_p2p_dht_remove}).  We thus infer that slower DHT lookup time causes the system to slowdown, and that OpenDHT scales somewhat poorly under higher load.

<%= figure 'pics/vr_multiples_take_1/client_download_Percentile_Line.pdf', :caption => 'P2P Multiple Files end download times', :label => 'figs:multiple_files_p2p_download_times_all_files' %>

<%= figure 'pics/vr_multiples_take_1/dht_Remove_Percentile_Line.pdf', :caption => 'P2P multiple files DHT key value removal times', :label => 'figs:multiple_files_p2p_dht_remove' %>

We also ran the same multiple file tests using a straight client-server download.  Download times grew at a much quicker rate (Fig. \ref{figs:multiple_files_cs_download_times}).  Server throughput appears to even slow with higher load (Fig. \ref{figs:multiple_files_cs_server_rate}).

<%= figure 'pics/vr_unnamed937328_multiple_files_cs/client_download_Percentile_Line.pdf', :caption => 'Client-server multiple files download times', :label => 'figs:multiple_files_cs_download_times' %>
<%= figure 'pics/vr_unnamed937328_multiple_files_cs/server_speed_Percentile_Line.pdf', :caption => 'Client-server multiple files server speed', :label => 'figs:multiple_files_cs_server_rate' %>

\subsection{Varying Block Size}
We next measure the effect block size has on downloads.  32KB blocks were shown to be effective in \cite{TODO}.  This also was shown in our own results, with 32K blocks resulting in the quickest downloads (Fig.  \ref{figs:block_size_download_times}), though not by much.

<%= figure 'pics/vr_unnamed240998_blockSize/client_download_Percentile_Line.pdf', :caption => 'Vary block size', :label => 'figs:block_size_download_times' %>

\section{Downloading Large Files}
We next want to download very large files, and compare ourselves to BitTorrent, famous for its speeds of large file download.  We download a single 30MB file from 100 simultaneous peers.  We use a block size of 256K for both protocols.  The expected result is that the two will fare similarly.  This wasn't exactly the case. BitTorrent shared a 30MB file fairly well.  Its median download time was 148.32s, and 99th percentiles was 2786.08s.  Our p2p protocol had a median download time of 847.1s, with 99th percentiles 982.67s.  One cause of this discrepancy is that BitTorrent seeds (servers) limit their outgoing connection count to a low number, thus enabling blocks to propagate more quickly from peer to peer.  In the BitTorrent test, the origin server wasn't an apache instance serving up to 256 clients, but limited its outgoing connections more far more aggressively.  
\footnote{Another difference between BitTorrent and our protocol is that we currently relook up the list of peers per block, even if our current peers are listed on more than one of those lists.  Some form of gossip to determine which known peers have which blocks might decrease network overhead.}
This test was of a flash flood downloading a large file, where our protocol falls short.  We would assume that in normal conditions there are a few well connected "seeder" peers entering the system before the flood occurs, in which case our protocol would work better.  We leave that examination for future work.

\section{Number of peers} We next perform the 30MB file download, varying the number of maximum number of peers to download from.  A higher number of concurrent peers increases download speed (Fig. \ref{figs:large_file_number_of_concurrent_blocks}).  
<%= figure 'pics/vr_vary_blocks_large_file/client_download_Percentile_Line.pdf', :caption => 'Vary number of concurrent blocks in a large file', :label => 'figs:large_file_number_of_concurrent_blocks' %>

\section{Current limitations}
Several options could help this protocol increase effectiveness which were not address.  For instance it could make smarter use of the DHT.  Currently it lists peers per block all as under a single key.  This could cause the owner of that single key to become overloaded if the crowd becomes too large.  Also it can cause staleness of the list if dead peers go offline without removing themselves from the list first (currently they issue a remove command when their linger time is up).  A better algorithm for this could be designed and tested, for example one that queries keys based on time (like one for peers within the last 10s, one for peers within the last 100s, etc).

We also currently connect to the origin from each peer. 

%Multiple file grew exponentially--we need to figure out a way to avoid that!
