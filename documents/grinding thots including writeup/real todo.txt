15's, 
fix [below]
normal fix [below the below]
120's
begin...the rollout dum dum dum dum...


run server on alice.cs.princeton.edu
distinct files [avoid some slowdown]
real apache
logs faster



attempt to 'debug' the weak hosts...is it Ruby?  If you 'do again' 12x_p2p_6 do they ever end up finishing?  
	Do the recreate bug on them...
try timeout on peers reading any bytes and also connect [sound familiar?] just hack it out
lodo an option 'don't serve until after you've downloaded it' -- this would help us see if it is a serving problem or what not.  or that option to turn off, try again.  Must work :)
TODO update ruby...
maybe patch Ruby...anything :)

lodo the honest answer is probably mini section like BT.  Then you can just swarm a block and feel good about yourself :) 'I got this chunk'

15x again with ALL including bad.  Still toast?
ignoring offender is ok for me for now.




normal debug: are they freezing? vlodo maybe 'super transfer mode' when in 'real' linger :)  Otherwise swap far more a lotta more to not lose conn's! :)
Is there perhaps a conflict with 'send' in one loop sending too much?  We pause for that, it resets TCP?






note: took out a few trouble peers that were nuking me.  Pieces of...



15x52 
pittsburgh had two one that HUNG there.  Very very very weird.  No log otput.  One after connecting to a p2p peer, then it started again.  This might be a computer problem. The others I don't know.
60x_1
planetlab2.arizona-gigapop.net hung 15 s 60x_1 or sending peer 216.165.109.82 hung [normal cleanup time]
planetlab2.cis.upenn.edu 31 s pause receiving from origin peer 48 60x_1
planet1.pittsburgh.intel-research.net slammed! took 97 to verify, (8 normally), 162 to download
planet1.pittsburgh.intel-research.net another took 223

60x_4
6 s 'pause' after a post opendht request for client 2 kinda slammed 32s verify
planet1.pittsburgh.intel-research.net 213 again [ugh]
planetlab2.arizona-gigapop.net 10s pauses

60x_cs 200 s approximately [we own that]

from 60x_5 versus 60x_cs_1 'seems that on certain threads either disk I/O or CPU will slow them down [cause pauses]'

from 60x_5thread_1 we can see that adding 5 threads makes it much more quickly.

from 60x_5thread_1 intel and also planet1.scs.nyu.edu some just...abort [seem to be fairly loaded machines]...umm...update Ruby, call it good.
	one after p2p client origin raising on p2p thread block 2

from 60x_5thread_2 [slower, for some reason, than the first--opendht slammed by simultaneous experiments, maybe?]
  still faster, converting on what we want.
  has some that show an opendht [just use CS on the server] problem.  Very odd.
  mostly opendht problems, and maybe last block if it's a slow peer.
[actually 4 threads, I believe]

60x_5thread_3 is actually 200 peers, at least 7 of whom froze

200x_5thread_3 many many froze, right?  But all on those 'lousy' hosts.  Something goes terribly terribly wrong, but only on certain hosts.  Debug on them?
from upenn I think some 'froze' at 30s
	froze: C:\thesis p2pwebcode\src\..\logs\planetlab2.cis.upenn.edu\200x_p2p_5threads_3_dClient_1_dTotal_200_dw_5.0_blockSize_100000_linger_3_dr_187500.0_dt_1_serverBpS_250000\peer_number_23_start_21.437.log.txt
200 s pause for after 396.607[13]: DEBUG:p2p server 47882:done (whatever reason) with peer conn 38 [sleep 1 then close]
wow
planetlab1.ucsd.edu crashes my peers a lot!
and planetlab2.ucsd.edu
planetlab2.cis.upenn.edu crashes my peers a lot, too
usually [see 200x_p2p_5threads_3_peer_number_145_start_141.031.log.txt_total_217.005_individual_graph.txt ]
the slowness is typically from some slow peer.  Or two.  On blocks.
sleep 200 on planet3.ucsd.edu after
	342.716[198]: DEBUG:p2p server 16954:done (whatever reason) with peer conn 9 [sleep 1 then close]

intel one...does some weird things to it

from peer_number_32_start_35.562.log.txt it seems that some peers connections just...reset after awhile.  Either from lack of CPU attention [ugh] or what not.

120x_p2p_5thread_8 seems to show convergence on 120S or so.
	lefthand.eecs.harvard.edu 'cancelled' one after 91.183[47]: DEBUG:p2p client whoa go and use peer early? block 4 already done! thread 0
	two from righthand died, too 
	planetlab1.ucsd.edu arbitrarily turfed it 96.530[96]: pre opendht request [http://128.112.139.80:30001/1000000_static3_0] block 0

compare with CS 400S average with 120 clients!


thot: in Ruby with threads you must tell it to swap a lot or it won't (?)