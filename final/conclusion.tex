\section{Conclusion}

We have shown how to successfully use a generic DHT to drive a scalable cache system for small files.
Our system has shown itself to be far more scalable than traditional content delivery on the Internet.
It is effective at alleviating load up on an origin web server for small files.

A few lessons were learned along the way, and many improvements are still possible.  
One is that OpenDHT can sometimes timeout for key lookups. %#Another problem with keys is that For example, sometimes 1 key out of 20 will take 20s, while the other 19 will all be very fast. 
This should be taken into account when building a system around OpenDHT.
With our system, we accomodated for the slow key lookup times by looking up several redundant keys from several redundant gateways,
as suggested by the authors of OpenDHT\cite{OpenDHT}.
%Unfortunately it appears that OpenDHT overloads easily when multiple
%keys are simultaneously requested from a single gateway. Some balance is yet to be found. Our protocol
%could accomodate for this better.
In our DHT the peer lists were served in order oldest to youngest. This can cause staleness in our system if peers, for example,
go offline without removing themselves from the list. This also causes peers to download the file
from the first peers listed, causing some unfairness\cite{Brian_Thesis}. A better algorithm for
this could be designed. Also, clients with a slow openDHT connection may receive peer lists that
are valid but have become outdated.

Another question is under what circumstances this protocol would actually be useful in the wild. Flash crowds
for servers that are limited by origin bandwidth would benefit, because static objects would
be cached (ex: a site offering a download that hasn't had a chance to propagate the new version to
mirrors yet--a common occurrence with slashdotted sites). However, those sites, because of the
increased traffic, may end up becoming limited by CPU, so our protocol might not be useful. 
Downloaders of very large files (ex: operating
system updates) would benefit, as the automatic P2P transition would alleviate load. 
%Groups of peers in close proximity that download a files could benefit (ex: within a large business several
%peers download the same file--it would be shared via local networks). 
A server that is cpu bound
would not benefit, since some pages are dynamic and thus not cacheable. 

\section{Future Work}
We currently do not handle well the case of a large flash crowd on a large file. If there is a small build-up
before a flash crowd then the system is primed better, however. We currently connect to the origin
once per peer. We could use some way to handle this better.

%We also do not examine the effectiveness of such a system for very poorly connected servers, such
%as dial-up modem users, nor did we examine if it is useful to a web site that is well connected. We do
%not attempt a true ``internet scale" test--i.e. many multiple files being requested simultaneously.

We currently use redundant requests for the last block, and also redundancy between
the origin server and peers for each block--there may be some way to reduce the waste caused by this
(BitTorrent also suffers from this).

We currently do not ensure the integrity of files (i.e. we assume peer trustworthiness and non file
corruption), nor do we provide peer incentives for sharing.

We leave these for future work.