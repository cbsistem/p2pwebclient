\section{Conclusion}

We have shown that a system of cooperating web clients can reduce load on a origin web server by automatically switching
from client-server file transfer to peer-to-peer content delivery. Our system has shown itself to be far more scalable 
than a traditional client-server download and can significantly
reduce download times.  The system is most effective for small files but does not yet compete well with
BitTorrent for large files.  We have also found good settings for various system parameters.  This system is mostly useful
for overloaded sites with static pages.

Several further optimizations are possible.  For example we currently connect to the origin
server once from each peer, causing the same blocks to be served (redundantly) to many peers in some instances.
This is wasteful in some instances.

We currently do not validate the integrity of files.  We assume peer trustworthiness and no file corruption.  
We also do not provide peer incentives for sharing.

Because peer lists are always returned in order from Bamboo, oldest to youngest, our system can experience staleness if peers
go offline without removing themselves from lists.  This ordering also causes peers to typically download the file
most from oldest listed peers, which can cause unfairness. This ordering can also cause peers to receive lists which
are outdated if they have a slow Internet connection.  A better algorithm for
could be designed than the polling one we use.

In our scalability studies, after a certain load the DHT response times increase.  There may be a way to avoid this.

With large files, over time clients do many requests for the different blocks.  These requests might all return the same
peer list currently.  This redundancy might be unnecessary.

If there is an extremely popular file, the requests for the peer lists might overwhelm the DHT member responsible
for that list.

Currently this system assumes static pages.  There could be some collaboration possible for peers 
could do here to store meta-data about which files are (or appear to be) static, and which are definitely dynamic.
This system only helps with bandwidth bound servers.  A way to also distribute CPU load to clients would be useful.

Currently we only use hard-coded system parameters, like $R$ and $b$, etc.  This may be non optimal, since it is hard
to guess the expected throughput.