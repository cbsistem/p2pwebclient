\section{Conclusion}

We have shown how to successfully use a generic DHT to drive a scalable web cache system, a useful research objective.  
A few outstanding lessons were learned.  One is that OpenDHT can be quite slow for certain keys, sometimes timing out after 90 or 200 seconds.  For example, sometimes 1 key out of 20 will take 20s, while the other 19 will all be very fast.  This should be taken into account when building a system around OpenDHT.  With our system, we accomodated for the slow key lookup times by looking up several redundant keys.  Unfortunately, from our multiple files test, it appears that OpenDHT overloads easily when multiple keys are simultaneously requested from a single gateway.  Some balance is yet to be found.  Our protocol could accomodate for this better.  OpenDHT serves peer lists in oldest to newest order, returning the oldest 9, then next oldest 9, etc.  This can cause staleness in our system if peers, for example, go offline without removing themselves from the list.  This also causes peers to download the file from the first peers listed, causing unfairness\cite{Brian_Thesis}.  A better algorithm for this could be designed.  Also, clients with a slow openDHT connection may receive peer lists that are valid but have become outdated\footnote{For example, in a system where all peers have a small linger time, it is possible that all lists received are already outdated.  As another example, if a client has a slow openDHT 'put' speed, its linger time could expire before it is even listed on the peer list, and therefore will always end up serving nothing.}.

Another question is under what circumstances this protocol would actually be useful.  Flash crowds on servers that are limited by the origin bandwidth would benefit, because static objects would be cached (ex: a site offering a download that hasn't had a chance to propagate the new version to mirrors yet--a common occurrence with slashdotted sites).  However, those sites, because of the increased traffic, may end up becoming limited by CPU.  Downloaders of very large files (ex: operating system updates) would benefit, as the automatic P2P transition would alleviate load.  Groups of peers in close proximity that download a files could benefit (ex: within a large business several peers download the same file--it would be shared via local networks).  A server that is cpu bound would not benefit, since root pages are typically dynamic and must be generated by the origin server for each request.  A server that is extremely bandwidth limited (ex: a DSL line, or cross-country connection) would benefit.  Overall it appears to be useful to both servers and downloaders, though not an end-all to all situations.

We currently do not handle well the case of a flash crowd with no warning.  If there is a small build-up before a flash crowd then the system is primed better, however.  We currently connect to the origin once per peer (i.e. from all peers).  We could use some way to handle this better, though it is presumed to be a rare case.

We also do not examine the effectiveness of such a system for very poorly connected servers, such as dial-up modem users, nor did we examine if it is useful to a web site that is well connected.  We do not attempt a true 'internet scale' test--i.e. many multiple files being requested simultaneously.

We currently use redundant requests currently for the last block, and also redundancy between the origin server and peers for each block--there may be some way to reduce the waste caused by this (BitTorrent also suffers from this).

We currently do not ensure the integrity of files (i.e. we assume peer trustworthiness and non file corruption), nor do we provide peer incentives for sharing.

We leave these for future work.
