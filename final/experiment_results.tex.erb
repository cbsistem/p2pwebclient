%% note: todo's go in todo, as well as optionally in here

\section{Experiment Results}
For our initial experiments we first test how well our system scales.  We download a small file (100KB) with an increasing number of clients to see how download times are affected by load.  We run the test with several different load settings.  In each test a number of peers enter the system within 100s.  Peers enter the system with a frequency given by poisson distribution with a rate such that, by 100s, they will all have entered the system.  The origin server is located on a well provisioned machine at BYU, using an apache2 web server that it bandwidth limited (using mod\_bw) to 256KB/s.  Peers are randomly selected from available PlanetLab hosts (a pool of up to 300 scattered world-wide on the PlanetLab system).  Experiments run until all peers finish downloading the file.  Each experiment is repeated 3 times and results are averaged.  Because a few peers are extremely poorly connected and create a long tail for aggregate statistics, measures of aggregate statistics ignore the last 20\% of the test time (since during that time the throughput is close to zero).  Because, at the beginning of tests, very few peers are downloading the file and it hasn't reached steady-state, the first 20\% is also ignored for aggregate statistics.  We measure the download times of all peers in a run, and display the 1st, 25th, 50th, 75th, and 99th percentiles.  We also measure the amount of load on the origin server, given by the sum of bytes received within a given second, by peers, from the origin.  Thus this only displacalculates bytes eceived from the origin, not necessary all that were sent, if some packets were lost.  We also calculate the percentage of the file received from peers versus from the origin, as well as percentiles of the time it took to do openDHT get, put, and remove commands over the course of each test.

We first test a traditional client-server transfer, to establish a base line from which to compare our system.  Fig.~\ref{figs:client_server_download_times} shows the download times for client-server download, as load increases.  It starts with a median download time of 1.33s with 1 peer entering the system/second, and quickly grows to a median of 344s at 25 peers/second.  Client-server download times increase almost linearly because we start a fixed number of peers and let them all complete their downloads.  Presumably if we continued the tests for longer we would see more of an exponential curve. The topmost outliers at 20s were peers which typically waited in line almost 900s before finally being served the file.  Fig.~\ref{figs:client_server_server_load} shows the load on the origin server over time.  It grows quickly to the theoretical cap of 250KB/s. then actually decreases to 203KB/s at 20 peers/second.  It shows the limitations of our bandwidth limiter in that it becomes bursty at higher connection rates, because it can't keep track of lost connections well.

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/client_download_Percentile_Line.pdf}
      \label{figs:client_server_download_times}      
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/server_speed_Percentile_Line.pdf}
      \label{figs:client_server_server_load}
    }
    \caption{Traditional client server download}  
  \end{center}
\end{figure*}


We next test our system under the same load.  For system parameters, we set reasonable defaults of block size 100KB, origin minimum allowable speed (R) 128KB/s, R's calculation window (W) 2s, and the first-byte timeout (T) of 1s.  Peers linger, serving the file, for 20 seconds after completing a download, and download from 5 peers at a time.  Download median times (Fig. \ref{figs:yanc_download_times}) start at 1.4s at a rate of 1 peer entering the system/second, and grow to 5.23 with 6 peers/s and to 7.46 with 20 peers/s.  Most peers took a few seconds to give up on the origin, query openDht and receive a peer list within about 5s after that, and download the file with the next few seconds, hence the median of about 7s.  Of those that took longer there were two basic causes.  One was that because they were only requesting one block, they requested the same block from up to 5 peers.  This caused some redundancy in bytes, which slowed down the transfer and caused the peers serving them the file to expire their linger times and go offline.  Thus they would have to requery openDHT, request the remainder of the file from more peers, and a similar pattern would repeat.  The other cause is that sometimes a request to OpenDHT would appear to be ignored or not responded to, so these peers would end up downloading the entire file from an (overloaded) origin, or would timeout the openDHT request and, when their next request was responded to, download the file at that point.  

Load on the origin appears to decrease with higher load, but in reality most of what the origin serves is being ignored by clients who have, while waiting for its receipt, downloaded the same bytes from other peers (Fig. \ref{figs:yanc_server_load}), so they ignore the incoming bytes in our measurements.

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/client_download_Percentile_Line.pdf}
      \label{figs:yanc_download_times}
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/server_speed_Percentile_Line.pdf}
      \label{figs:yanc_server_load}
    }
    
    \subfigure[CDF of percent of file received from peers] {
      \includegraphics[width=70mm,]{pics/vr_medium_p2p_load_tak4/percent_from_clients_Percentile_Line.pdf}
      \label{figs:yanc_from_client_percentile}
    }
    
    \caption{P2P Download}    
  \end{center}
\end{figure*}

Also related is the increasing amount of P2P transfer that occurs under higher loads.  Under low load peers tend to download the file directly from the origin, however, after about 10 peers/s almost 100\% of the transfer is from peers (Fig.~\ref{figs:yanc_from_client_percentile}).  A 1 on the graph means that 100\% of the file transfer was from peers, instead of from the origin.  The reason it isn't always at 100\% for lower load is that peers typically start downloading the file from the origin, then after 2s they would deem it to be serving less than R (128KB/s) and switch to P2P for the remainder of the file.  The lowest percentile almost always download the entire file from the origin (typically the first few peers to enter the system), and the highest percentile download all the file via p2p, typically the last peers to enter the system when the origin is already overloaded.

\section{Impact of various parameters}

Theoretically, if a peer can download from a fast origin,  it does not need to try to download the same file from peers.  Our system allows for this by specifying parameters for the conditions under which it will deem the origin too slow and switch to p2p download \footnote{It actually switches to a mixed download, since it still downloads from the origin server if no peers are available.}.

We next test the impact of varying these parameters, in order to explore the dynamics of the system, and search for optimal values.  In each test we run 1000 peers, with 15 entering the system per second, and wait for all peers to finish downloading the file, then compare download times.  We repeat each test twice.  The default settings are to download a 100KB file, with a block size of 100KB, from 5 peers simultaneously, with T set to 1s, R set to 128KB/s, and W set to 2s.

\subsection{Varying time to wait for first byte}

We first vary $T$, the timeout for waiting for a first response byte from the origin server\footnote{By first byte we mean any byte received from the origin server--a header byte or a content byte--i.e. we exclude the TCP handshake packet as a first byte.}.  
We expect that an extremely low value will cause transitioning too early and a too high value will cause transitioning too late.  The results were that with a T setting of 0, median download time was 12s, it then dropped to 4.5s when set to 0.75s, then gradually rose to 10s at 2, and flattened out for higher values of T (Fig. \ref{figs:dt_variance_download_times}). The reason it flattened out is that as T grew (instead of continuing to grow, as we expected), was that T's relative importantce decreased with higher load, as R caused the transition more often than T did (Fig. \ref{figs:dt_variance_death_reasons}).  The outliers for download time were usually peers who were poorly connected to OpenDHT, so they sometimes received answer to their queries after the linger time for the peers listed had expired.

<%= figure 'pics/vr_do_dts_take6/client_download_Percentile_Line.pdf', :label => 'figs:dt_variance_download_times', :caption => 'Download times varying T, the time to wait for first byte' %>

<%= figure 'pics/vr_do_dts_take6/death_reasons.pdf', :label => 'figs:dt_variance_death_reasons', :caption => 'Reasons peers switched to P2P download, varying T' %>

%\subsection{Server minimum allowed speed R}

%We next vary $R$, the speed of receipt at which we transition to P2P download.  This value is calculated by averaging the amount received over the previous W seconds.  We run this experiment by varying R from 32KB/s to 1MB/s, and leave T set to 1s, a reasonable value.  The expected result was that a value of R too high would cause transition too early, and that a value too low would cause a transition too late. 

%TODO rerun the dR test, turns out it was a corrupt run, somehow...

\subsection{Server speed time window W}

We next vary W, the seconds of recent history used to calculate R.  Our hypothesis is that a too small W will be too sensitive and thus inaccurate.  We varied W from 0.1s to 10s, set T to 10s, so that we could better see the impact of R, and ran the same experiments desribed above (1000 peers within 100s until completion).  The results were that it did make some difference, but only when set to a very low value.  Download times were their lowest at W set to 0.25s, of 17s, then rose to about 25s for the rest of the settings (Fig.~/ref{figs:dw_download_times}).  Surprisingly, most peers (consistently between 770 and 810 out of 1000) still transitioned to a P2P download because of a slow first byte (T), even with T set to the larger value of 10s (Fig.~/ref{figs:dw_death_reason}).

% ltodo so was it all the first 200 that always transitioned to R, so that's what sped things up a bit?

<% figure 'pics/837145_dw/client_download_Percentile_Line.pdf', :label => 'figs:dw_download_times', :caption => 'Download times by varying W' %>

<% figure 'pics/837145_dw/death_reasons.pdf', :label => 'figs:dw_death_reasons', :caption => 'Download times by varying W' %>

\section{Effect for a full web page}
We next run an experiment more indicative of real-world situations.  The normal pattern for a webpage is to first access some root page which references several other objects, such as images, javascript, flash, etc.  This is the case for the BYU web site, which has a medium sized main page that links to over 10 small other objects\footnote{Snapshot 2007}.  We therefore run an experiment to see how well our system downloads a small page followed by several smaller objects.  Each peer first downloads a 100K file.  When that file completes each peer then downloads 10 10K files simultaneously (all from the origin server), and the total time to download all 11 files is calcualted.  We set the parameters to be those mentioned in the base test, above, and run 1000 peers, varying the peer entry times from 1/s to 25/s.

The expected result is that downloading multiple pages instead of just one will be slower than downloading one since it will put more stress on the DHT, which proved correct (Fig. \ref{figs:multiple_files_p2p_download_times_all_files}).  Total download time starts at 1.3s, and appears to grow exponentially, reaching 146s at 25 peers/s.  Also correlated is that DHT set times also grow, starting at a median of 0.41s at 1 peer/s and growing to a median of 12s at 25 peers/s (Fig. \ref{figs:multiple_files_p2p_dht_put}).  We thus infer that the way we use OpenDHT scales somewhat poorly under higher load.

% todo is that what really happen though?

%TODO subfigure these...

<%= figure 'pics/vr_multiples_take_1/client_download_Percentile_Line.pdf', :caption => 'Download times for all files', :label => 'figs:multiple_files_p2p_download_times_all_files' %>

<%= figure 'pics/vr_multiples_take_1/dht_Put_Percentile_Line.pdf', :caption => 'Multiple files DHT set times', :label => 'figs:multiple_files_p2p_dht_put' %>

We next run the same test using traditional client-server download.  Download times grew at a much steeper curve (Fig. \ref{figs:multiple_files_cs_download_times}).  Ours scaled much better than client-server.

<%= figure 'pics/multiples_p2p_versus_cs_pics/client_download_Percentile_Line.pdf', :caption => 'Comparison of P2P versus client server download times for multiple files', :label => 'figs:multiple_files_cs_download_times' %>

\subsection{Varying Block Size}
We next measure the effect block size has on downloads.  We download  a 100K file with different block sizes.  32K blocks resulted in the quickest downloads (10.6s median), (Fig.~\ref{figs:block_size_download_times}).

%32KB blocks were shown to be effective in \cite{TODO}.  

<%= figure 'pics/vr_unnamed240998_blockSize/client_download_Percentile_Line.pdf', :caption => 'Download times varying block size', :label => 'figs:block_size_download_times' %>

\section{Downloading Large Files}
We next test our system downloading large files.  We download a single 30MB file from 100 peers with our system (settings as described above) and then download the same file with BitTorrent and compare download times.  Block size is set to 256K for both systems (the BitTorrent default).  Peers enter the system at an average of 1/s, and both protocols' origin servers are rate limited at 256KB/s.  This means that peers were entering at 120x the actual capacity of the origin server.  The expected result is that the two fare similarly, though it turned out that some peers were faster with ours, and some with BitTorrent (Table ~\ref{figs:yanc_vs_bt}).  Ours did more poorly for the median (847s to 148s), and BitTorrent worse for the 99th percentile (2786s to 847s).

One thing to note is how close the download percentiles of each protocol are. The difference between 25th and 99th percentiles of our system is about 150s, or 6\%.  The difference between the 1st and 75th percentiles of BitTorrent is 32s, or 5\%.  This implies that once the file enters the system in its entirety (i.e. one peer has it), it disseminates relatively quickly to all the other peers, in both our system and BitTorrent.  Our system doesn't disseminate a complete copy as quickly, though most peers received the file from other peers (Fig.~\ref{figs:yanc_30mb_cdf}).  One reason for this is that BitTorrent's seed (origin server) limits the number of outgoing connections it allows (apache--our server--does too, but at a much higher limit).  Limiting this more enables BitTorrent to propagate blocks more quickly to connected peers and through the system.  BitTorrent seeds also favor peers who have higher download speeds, which speeds propagation of blocks from the origin.  These differences allow it to respond to a flash crowd such as this one better than ours.

<%= figure 'pics/yanc_30mb/yanc_30_mb_cdf.pdf', :caption => 'CDF of percent of file received from peers, 30MB file', :label => 'figs:yanc_30mb_cdf' %>

\begin{table}
  \caption{Download times of a 30MB file}
\begin{tabular}{ c c l }
  Percentiles & Ours (s) & BitTorrent (s) \\
  \hline
  1 & 613 & 131 \\
  25 & 730 & 139 \\
  50 & 847 & 148 \\
  75 & 882 & 163 \\
  99 & 982 & 2786 \\
  \label{figs:yanc_vs_bt}
\end{tabular}
\end{table}
  
\section{Large File: Number of peers} We next vary the maximum number of peers that each peer downloads from.  We use the same 30MB file size from the previous experiment, and vary the peer limit from 1 to 50.  We expect that download times will suffer if the connection limit is too low.  As expected, median download times were higher with a smaller connection limit, with a peak time of 1604s with limit set to 1.  As expected, download speed increased with a higher connection limit, with a 16 peer limit yielding a median download time of 931s (Fig. \ref{figs:large_file_number_of_concurrent_blocks}), however a number greater than 16 didn't yield as high of gains, with a 50 peer limit resulting in a median download time of 875s. 

<%= figure 'pics/vr_vary_blocks_large_file/client_download_Percentile_Line.pdf', :caption => 'Download times varying number of concurrent peers', :label => 'figs:large_file_number_of_concurrent_blocks' %>