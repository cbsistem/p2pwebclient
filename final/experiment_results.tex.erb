%% note: todo's go in todo, as well as optionally in here

\section{Experiment Results}
For our initial experiments we test how well our system scales.  We download a small file (100KB) under several different load settings.  In each test a given number of peers enter the system within the first 100s, using a poisson distribution to make entry times more random, but still release peers into the system by 100s.  Peers all download the file from an origin server located on a well provisioned machine at BYU, using an apache2 web server that it bandwidth limited (using mod\_bw) to 256KB/s.  Peers are randomly selected from available PlanetLab hosts (a pool of up to 300 scattered world-wide on the PlanetLab system).  Experiments run until all peers finish downloading the file.  Each experiment is repeated 3 times and results are averaged.  A different filename is used with each run, so as to not reuse the same DHT keys.  Because a few peers are extremely poorly connected and create a long tail for aggregate statistics, measures of aggregate statistics ignore the last 20\% of the run (since during that time the throughput is close to zero).  Also, because very few peers are downloading the file at the very beginning of tests, and the system hasn't reached steady-state, the first 20\% is ignored for aggregate statistics.  We collect the download times of all peers in a run, and calculate the 1st, 25th, 50th, 75th, and 99th percentiles.  We also measure the amount of load on the origin server, given by the sum of bytes received within a given second from the origin.  This only measures bytes received from the origin, not bytes sent, as any data received after a socket has already been closed is ignored by TCP, and peer sometimes receive a file from a peer and  close their connection with the server.  We also calculate the percentage of the file received from peers versus from the origin, as well as the time required to do openDHT get, put, and remove operations over the course of each run.

We first test a traditional client-server transfer, to establish a base line from which to compare our system.  Fig.~\ref{figs:client_server_download_times} shows the download times for client-server download, varying load.  It starts with a median download time of 1.33s with 1 peer entering the system/second, and quickly grows to a high of 344s at 20 peers/second.  Client-server download times increase linearly because we start a fixed number of peers and let them all complete their downloads.  Presumably if we continued the tests for longer we would see more of an exponential curve. The topmost outliers at 20s were peers which typically waited in line almost 900s before finally being served the file.  Load on the origin server grew quickly to its theoreticaly cap of 250KB/s (Fig.~\ref{figs:client_server_server_load}).  With higher load (20 peers/s) it actually decreases to 203KB/s, which shows the limitations of our bandwidth limiter in that it becomes bursty at higher connection rates.

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/client_download_Percentile_Line.pdf}
      \label{figs:client_server_download_times}      
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_unnamed316651_cs_stress_test/server_speed_Percentile_Line.pdf}
      \label{figs:client_server_server_load}
    }
    \caption{Traditional client server download}  
  \end{center}
\end{figure*}


We next test our system under the same loads.  For system parameters, we set reasonable defaults of block size 100KB, origin minimum allowable speed (R) of 128KB/s, R's calculation window (W) of 2s, and the first-byte timeout (T) of 1s.  Peers linger, serving the file, for 20 seconds after completing a download, and download from at most 5 others peers at a time.  Download median times (Fig. \ref{figs:yanc_download_times}) start at 1.4s at a rate of 1 peer entering the system/second, and grow to 5.23s at 6 peers/s and 7.46s at 20/ peers/s.  Most peers took 2 seconds (W) to give up on the origin server, because they measured its speed as less than 128KB/s (R).  They then query OpenDht for a peer list and receive about 5s after that, then download the file almost immediately, hence the median of 7s.  Of those that took longer there were two basic causes.  One was that a few slow peers would take long enough to download a file that the linger times of their peers would expire.  When their peers would go offline, they would have to request a new list of peers, causing an increase in latency.  This problem was exacerbated by the fact that we request the last block from multiple peers, in order to download the last block more efficiently.  This causes some redundancy in bytes received, which slows down slow peers, causing them to timeout their peer connections more often.  The other factor causing slowdown is that sometimes a request to OpenDHT would be ignored, and peers would end up downloading the entire file from an (overloaded) origin, or timeout the unresponsive OpenDHT request after 60s, perform a new query, and then quickly download the file within 7s after that.

Load on the origin appears to decrease with higher load, but in reality most of what the origin serves is being ignored by clients who have, while waiting for the bytes from the origin, downloaded the file already from their peers (Fig. \ref{figs:yanc_server_load}).

\begin{figure*}
  \begin{center}
    \subfigure[Download times]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/client_download_Percentile_Line.pdf}
      \label{figs:yanc_download_times}
    }     
    \subfigure[Load on the origin server]{
      \includegraphics[width=7cm]{pics/vr_medium_p2p_load_tak4/server_speed_Percentile_Line.pdf}
      \label{figs:yanc_server_load}
    }
    
    \subfigure[CDF of percent of file received from peers] {
      \includegraphics[width=70mm,]{pics/vr_medium_p2p_load_tak4/percent_from_clients_Percentile_Line.pdf}
      \label{figs:yanc_from_client_percentile}
    }
    
    \caption{P2P Download}
  \end{center}
\end{figure*}

The amount of P2P transfer also increases under higher loads.  Under low load peers tend to download the file only from the origin, however, after about 10 peers/s almost 100\% of the transfer is from peers (Fig.~\ref{figs:yanc_from_client_percentile}--A 1 on the graph means 100\% of the file transfer was from peers).  The reason it isn't always at 100\% for lower load levels is that peers typically start downloading the file from the origin, receive some portion of it within the first W (2) seconds, then transition to P2P and receive the rest.  The fastest percentile almost always download the entire file from the origin (typically the first few peers to enter the system), and the highest percentile almost always download the file using p2p, especially peers who encounter a bogged down origin.

\section{Impact of various parameters}

Theoretically, if a peer can download from a fast origin,  it does not need to try to download the file from peers.  Our system allows for this by specifying parameters for the conditions under which it will deem the origin too slow and switch to p2p download \footnote{It actually switches to a mixed download, since it still downloads from the origin server if no peers are available.}.  We next test the impact of varying these parameters, in order to explore the dynamics of the system.  In each test we run 1000 peers, with 15 entering the system per second, and wait for all peers to finish downloading.  We repeat each test twice.  The default settings are to download a 100KB file, with a block size of 100KB, from at most 5 peers simultaneously, with T set to 1s, R set to 128KB/s, and W to 2s.

\subsection{Varying time to wait for first byte T}

We first vary $T$, the timeout for waiting for a first response byte from the origin server\footnote{By first byte we mean any byte received from the origin server--a header byte or a content byte--i.e. we exclude the TCP handshake packet as a first byte.}.  
We expect that an extremely low value will cause transitioning too early and extremely high values will cause transitioning too late.  The results held out our hypothesis.  With a T setting of 0, median download time was 12s, it then dropped to 4.5s when set to 0.75s, then gradually rose to 10s at 2s, and flattened out for higher values of T (Fig. \ref{figs:dt_variance_download_times}). The reason it flattened out with higher values for T was that T's relative importance decreased with higher load, as R caused the transition more often than T when T was set high (Fig. \ref{figs:dt_variance_death_reasons}).  With low values of T, almost 100\% of peers transitioned to P2P download because of it.  With T at 10 seconds about 50\% of peers transitioned because of T, and 50\% because of R.  The outliers for download time were usually peers who were poorly connected to OpenDHT, so they sometimes received answers to their queries after the answers were already outdated, so had trouble connecting to live peers.

<%= figure 'pics/vr_do_dts_take6/client_download_Percentile_Line.pdf', :label => 'figs:dt_variance_download_times', :caption => 'Download times varying first byte timeout (T)' %>

<%= figure 'pics/vr_do_dts_take6/death_reasons.pdf', :label => 'figs:dt_variance_death_reasons', :caption => 'Cause of transition to P2P download, varying T' %>

%\subsection{Server minimum allowed speed R}

%We next vary $R$, the speed of receipt at which we transition to P2P download.  This value is calculated by averaging the amount received over the previous W seconds.  We run this experiment by varying R from 32KB/s to 1MB/s, and leave T set to 1s, a reasonable value.  The expected result was that a value of R too high would cause transition too early, and that a value too low would cause a transition too late. 

%TODO rerun the dR test, turns out it was a corrupt run, somehow...

\subsection{Server speed time window W}

We next vary W, the seconds of recent history used to calculate R.  Our hypothesis is that a too small W will be too sensitive and thus inaccurate.  We varied W from 0.1s to 10s and ran the same experiments as above (1000 peers within 100s until completion).  We also set T to a higher value of 10s, so that R would be a more instrumental in causing the transition to P2P download.  The results were slightly different than our hypothesis.  Varying W did make some difference, but only when set to a very low value.  Median download times were 17s with W set to 0.25s.  For W \textgreater{} 0.25s most download times were around 25s (Fig.~\ref{figs:dw_download_times}).  Surprisingly, most peers (consistently about 800 out of 1000) still transitioned to a P2P download because of a slow first byte (T), even with T set to the larger value of 10s (Fig.~\ref{figs:dw_death_reasons}).  The reason for this is that we don't start calculating R until after the first byte is received, and in the majority of cases this didn't happen until after T had expired, at 10s, so T still causes most of the P2P transitioning.  You'll notice that the median download time of 17s for these runs is essentially the same as the initial load tests (7s) + 10.

% ltodo so was it all the first 200 that always transitioned to R, so that's what sped things up a bit?

<%= figure 'pics/837145_dw/client_download_Percentile_Line.pdf', :label => 'figs:dw_download_times', :caption => 'Download times by varying W' %>

<%= figure 'pics/837145_dw/death_reasons.pdf', :label => 'figs:dw_death_reasons', :caption => 'Download times by varying W' %>

\section{Effect for a full web page}
We next run an experiment more indicative of real-world situations.  The normal pattern for a webpage is to first access some root page which references several other objects, such as images, javascript, flash, etc.  This is the case for the BYU web site, which has a medium sized main page that links to over 10 small other objects\footnote{Snapshot 2007}.  We therefore next run an experiment to see how well our system downloads a small page followed by several smaller files.  Each peer first downloads a 100K file.  When that file completes each peer then downloads 10 10K files simultaneously (all from the origin server), and the total time to download all 11 files is measured.  We set the parameters to be those mentioned in the base test, above, and run 1000 peers, varying the load on the origin from 1 peer/s to 25/s.

The expected result is that downloading multiple pages will be slower than downloading one since it will put more stress on the DHT, which proved correct (Fig. \ref{figs:multiple_files_p2p_download_times_all_files}).  Total download time starts at 1.3s, and appears to grow exponentially, approaching 150s total download time at 25 peers/s.  Also correlated is that DHT times grow with load, starting at a median of 0.41s at 1 peer/s and growing to a median of 12s/query at 25 peers/s (Fig. \ref{figs:multiple_files_p2p_dht_put}).  Because we are download 11 files instead of 1, this causes the sum download time to increase.

% todo is that what really happens though?

%TODO subfigure these...

<%= figure 'pics/vr_multiples_take_1/client_download_Percentile_Line.pdf', :caption => 'Download times for all files', :label => 'figs:multiple_files_p2p_download_times_all_files' %>

<%= figure 'pics/vr_multiples_take_1/dht_Put_Percentile_Line.pdf', :caption => 'Multiple files DHT set times', :label => 'figs:multiple_files_p2p_dht_put' %>

We next re-run the same test using a traditional client-server download.  Download times grew at a much steeper curve (Fig. \ref{figs:multiple_files_cs_download_times}).  Ours scaled much better than client-server, despite scaling somewhat less than hoped.

<%= figure 'pics/multiples_p2p_versus_cs_pics/client_download_Percentile_Line.pdf', :caption => 'Comparison of P2P versus client server download times for multiple files', :label => 'figs:multiple_files_cs_download_times' %>

\subsection{Varying Block Size}
We next measure the effect block size has on downloads.  We download  a 100K file with different block sizes.  32K blocks resulted in the quickest downloads (10.6s median) (Fig.~\ref{figs:block_size_download_times}).

%32KB blocks were shown to be effective in \cite{TODO}.  

<%= figure 'pics/vr_unnamed240998_blockSize/client_download_Percentile_Line.pdf', :caption => 'Download times varying block size', :label => 'figs:block_size_download_times' %>

\section{Downloading Large Files}
We next test our system downloading large files.  We download a single 30MB file with 100 peers, using the same settings as the initial tests, and then download the same file with 100 peers, using BitTorrent.  Block size is set to 256K for both systems (the BitTorrent default).  Peers enter the system at an average of 1/s, and both protocols' origin servers are rate limited at 256KB/s (120x the actual capacity of the origin server).  The expected result is that the two fare similarly.  It turned out that a few peers downloaded faster with our system, but most download faster with BitTorrent (Table~\ref{figs:yanc_vs_bt}).  Ours did worse for the median than BitTorrent (847s to 148s), and BitTorrent was worse for the 99th percentile (2786s to 847s).  One reason for the difference in download speed is that BitTorrent's seed (origin server) limits the number of outgoing connections it allows (apache--our server--does too, but at a much higher limit).  This enables BitTorrent to propagate blocks more quickly to a few peers who then spread the blocks among the other peers.  BitTorrent seeds also favor peers who have higher download speeds, which speeds propagation of blocks from the origin.  These differences allow it to respond to a flash crowd such as this one better than ours.  One thing to note is how close the download percentiles within each protocol are. The difference between 25th and 99th percentiles of our system is about 150s, or 5\%.  The difference between the 1st and 75th percentiles of BitTorrent is 32s, or also about 5\%.  This implies that once the file enters the system in its entirety (i.e. once a single peer has it), it disseminates relatively quickly to all the other peers, in both our system and BitTorrent.  Most peers received the file from other peers (Fig.~\ref{figs:yanc_30mb_cdf}), with only 2\% of peers receiving more than 50\% of the file from the origin.  Percentage from peers grows after about 5 peers to 80\% of the file, and 50\% of peer received 97\% or more of the file from their peers.

<%= figure 'pics/yanc_30mb/yanc_30_mb_cdf.pdf', :caption => 'CDF of percent of file received from peers, 30MB file', :label => 'figs:yanc_30mb_cdf' %>

\begin{table}
  \caption{Download times of a 30MB file}
\begin{tabular}{ c c l }
  Percentiles & Ours (S) & BitTorrent (S) \\
  \hline
  1 & 613 & 131 \\
  25 & 730 & 139 \\
  50 & 847 & 148 \\
  75 & 882 & 163 \\
  99 & 982 & 2786 \\
  \label{figs:yanc_vs_bt}
\end{tabular}
\end{table}
  
\section{Peer connection limit} We next vary the maximum number of connections each peer will make to other peers.  We download the same 30MB file size from the previous experiment, using 100 peers, and vary the peer connection limit from 1 to 50.  We expect that download times will suffer if the connection limit is set too low.  As expected, median download times suffered with a low connection limit, with a peak time of 1604s.  Download speed increased with a higher connection limit, with a limit of 16 yielding a median download time of 931s (Fig. \ref{figs:large_file_number_of_concurrent_blocks}), however limits greater than 16 didn't yield as high of relative gains.  A 50 peer limit resulted in a median download time of 875s.

<%= figure 'pics/vr_vary_blocks_large_file/client_download_Percentile_Line.pdf', :caption => 'Download times varying number of concurrent peers', :label => 'figs:large_file_number_of_concurrent_blocks' %>